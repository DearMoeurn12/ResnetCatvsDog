{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DearMoeurn12/ResnetCatvsDog/blob/main/dogs-vs-cats%20-with-Resnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZeOYNZDygnJ"
      },
      "source": [
        "<a href=\"https://imgbb.com/\"><img src=\"https://i.ibb.co/YcGVHgv/Picture2.png\" alt=\"Picture2\" border=\"0\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82jySSiuygnP"
      },
      "source": [
        "## Build ResNet from Scratch With Python !\n",
        "\n",
        "**Dogs VS Cats Classification With Resnet**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbKwSWY0ygnP"
      },
      "source": [
        "The Dogs & Cats is a foundational problem for a basic CNN(convolutional neural network) model which involves classifying images as a dog or a cat.The dataset can be used for learning how to develop,evaluate and use convolutional deep learning neural networks for classification of images. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61YmKQG7ygnQ"
      },
      "source": [
        "The dataset can be downloaded for free from the Kaggle website.\n",
        "Download the dataset by visiting the Dogs vs. Cats Data page and click the “Download All” button.\n",
        "Dataset Link : https://www.kaggle.com/c/dogs-vs-cats\n",
        "This will download the 850-megabyte file “dogs-vs-cats.zip” to your workstation.\n",
        "Unzip the file and you will see train.zip, train1.zip and a .csv file. Unzip the train.zip file, as we will be focusing only on this dataset.\n",
        "You will now have a folder called ‘train/‘ that contains 25,000 .jpg files of dogs and cats. The photos are labeled by their filename, with the word “dog” or “cat“. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## install libarys\n",
        "!pip install tensorflow\n",
        "!pip install opencv-python\n",
        "!pip install -U scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRKYBShlTcYK",
        "outputId": "f78f88be-8d1b-478e-dbf4-656e52f4700c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.8.2+zzzcolab20220719082949)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.21.6)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.26.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (4.1.1)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow) (57.4.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (14.0.6)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.17.3)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.2.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.0.7)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.47.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow) (1.5.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.35.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (3.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (3.2.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (4.6.0.66)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python) (1.21.6)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (1.0.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.7.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.21.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# laod data forom kagggle\n",
        "! pip install -q kaggle\n"
      ],
      "metadata": {
        "id": "j5BH1ZCNdjwY"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "_RKUajQ1d0-D",
        "outputId": "4fb0af8f-6e0a-477a-9388-31a3a72e15a2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ac49e2f4-1136-412f-9819-4dc9e6fb847d\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-ac49e2f4-1136-412f-9819-4dc9e6fb847d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-5c2e8a8d365b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m   \"\"\"\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m   \u001b[0muploaded_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_upload_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultiple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m   \u001b[0;31m# Mapping from original filename to filename as saved locally.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m   \u001b[0mlocal_filenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36m_upload_files\u001b[0;34m(multiple)\u001b[0m\n\u001b[1;32m    116\u001b[0m   result = _output.eval_js(\n\u001b[1;32m    117\u001b[0m       'google.colab._files._uploadFiles(\"{input_id}\", \"{output_id}\")'.format(\n\u001b[0;32m--> 118\u001b[0;31m           input_id=input_id, output_id=output_id))\n\u001b[0m\u001b[1;32m    119\u001b[0m   \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_collections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result, timeout_sec)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    100\u001b[0m         reply.get('colab_msg_id') == message_id):\n\u001b[1;32m    101\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: TypeError: Cannot read properties of undefined (reading '_uploadFiles')"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftiQ3hBjygnQ"
      },
      "source": [
        "**Import Library**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "IOjc3oYYygnR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import random\n",
        "import shutil\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "from keras.models import load_model\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.image import imread\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator,load_img\n",
        "from keras.models import Sequential,Model\n",
        "from keras.layers import Conv2D,Flatten,MaxPooling2D,Dense,GlobalAveragePooling2D,Dropout,BatchNormalization\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGUe6pTAygnT"
      },
      "source": [
        "**Prepare Training Data**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! kaggle datasets download -d dearm12/catvsdog"
      ],
      "metadata": {
        "id": "Emc6ekGQ2Zcs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8044b506-106f-4a38-e3ce-0a924cf5cf83"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/kaggle\", line 5, in <module>\n",
            "    from kaggle.cli import main\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/kaggle/__init__.py\", line 23, in <module>\n",
            "    api.authenticate()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/kaggle/api/kaggle_api_extended.py\", line 166, in authenticate\n",
            "    self.config_file, self.config_dir))\n",
            "OSError: Could not find kaggle.json. Make sure it's located in /root/.kaggle. Or use the environment method.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPh9oAmkygnT"
      },
      "source": [
        "Now our dataset contains only images of dogs and cats with filenames like 'dog.1.png' or 'cat.255.png'. Here we have prepared our label dataset by splitting image name with '.'  eg  if image name is 'dog.1.png' we split it by '.' as delimiter and extract first part of image name ('dog' here ) and if it is dog the we append 1 in list else we append 0 in list.So now basically we have our dataset comprising of images and labels corresponding to each images \n",
        "* if label = 1 , image corresponds to dog and \n",
        "* label = 0 if image corresponds to cat.\n",
        "\n",
        "We have stored filenames and labels using pandas dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "aHeVcle3ygnU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "5a1d7102-3ebb-477d-863d-01129d7ad88f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-8d7edea5f7f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/train\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mfilenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/train'"
          ]
        }
      ],
      "source": [
        "Y = []\n",
        "path = \"/train\"\n",
        "filenames = os.listdir(path)\n",
        "for img in os.listdir(path):\n",
        "    val = img.split(\".\")[0]\n",
        "    if val == \"dog\":\n",
        "        Y.append('1')\n",
        "    else:\n",
        "        Y.append('0')\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    'filename' : filenames,\n",
        "    'category' : Y\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFzZrbS6ygnV"
      },
      "source": [
        "Here we see few initial values of our dataframe using df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44LDOhWTygnV"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gL8appFfygnW"
      },
      "source": [
        "Here we see few last values of our dataframe using df.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HsLROG35ygnW"
      },
      "outputs": [],
      "source": [
        "df.tail()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4PE90PpygnW"
      },
      "source": [
        "**Plot Random Dataset Image**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXKvGLdaygnX"
      },
      "source": [
        "Looking at a few random images in the directory, we can see that the images are coloured and have different shapes and sizes. For example, let’s load and plot some random images of dataset in a single figure. Running below code creates a figure showing some random images from the dataset.We can see that some images are landscape format, some are portrait format, and some are square."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "tbIJAp8KygnX"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 12))\n",
        "for i in range(0, 9):\n",
        "    plt.subplot(4, 3, i+1)\n",
        "    sample = random.choice(filenames)\n",
        "    filename = path+'/'+sample\n",
        "    image = imread(filename)\n",
        "    plt.imshow(image)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j86HdPYzygnX"
      },
      "source": [
        "**Train Test Split**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_B1MnnESygnY"
      },
      "source": [
        "Here we are splitting our dataset into two parts. We are separating 20% of our dataset images for validation purpose and keeping rest for training purpose.This is done because we want to validate our trained model on a new set of images and not on the same set of images on which the model is trained. Therefore we want a different set of images apart from our training set upon which we can test our model and see the performance of the trained model on new unseen images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gnhyQgh8ygnY"
      },
      "outputs": [],
      "source": [
        "train_df,test_df = train_test_split(df,test_size=0.2,random_state = 42)\n",
        "train_df = train_df.reset_index(drop=True)\n",
        "test_df = test_df.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AOIBkD5QygnY"
      },
      "outputs": [],
      "source": [
        "print(test_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zC2AUSnygnY"
      },
      "source": [
        "**Training Dataframe shape**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dr6p5oXLygnZ"
      },
      "source": [
        "To see our training dataframe shape we are using train_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rRWyHbDYygnZ"
      },
      "outputs": [],
      "source": [
        "train_df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZad4L2VygnZ"
      },
      "source": [
        "**Validation Dataframe shape**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYDDZVVMygnZ"
      },
      "source": [
        "To see our training dataframe shape we are using val_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w7aVZm8CygnZ"
      },
      "outputs": [],
      "source": [
        "test_df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNc8s5U8ygnZ"
      },
      "source": [
        "Here we are defining constants to be used in our model code. So as to make our code modular we define all constants in a separate cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TlhgXW0Tygna"
      },
      "outputs": [],
      "source": [
        "train_size = train_df.shape[0]\n",
        "test_size = test_df.shape[0]\n",
        "img_hieght = 64\n",
        "img_width = 64\n",
        "img_channels = 3\n",
        "batch_size = 32"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xq0aPkgqygna"
      },
      "source": [
        "**Training Generator**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izrloSy0ygna"
      },
      "source": [
        "Image data augmentation is a technique that can be used to artificially expand the size of a training dataset by creating modified versions of images in the dataset.Training deep learning neural network models on more data can result in more skillful models, and the augmentation techniques can create variations of the images that can improve the ability of the fit models to generalize what they have learned to new images. Data augmentation can also act as a regularization technique, adding noise to the training data, and encouraging the model to learn the same features, invariant to their position in the input. Small changes to the input photos of dogs and cats might be useful for this problem, such as small shifts and horizontal flips. These augmentations can be specified as arguments to the ImageDataGenerator used for the training dataset. The augmentations should not be used for the validation dataset, as we wish to evaluate the performance of the model on the unmodified photographs.                    \n",
        "We can use the flow_from_dataframe() function on the data generator and create one iterator for each of the train and validation set. With the help of flow_from_dataframe method you can directly pass the Pandas DataFrame which has the mapping between filenames of the images and their labels..We must specify that the problem is a binary classification problem via the “class_mode” argument, and to load the images with the size of 128×128×3 pixels via the “target_size” argument. We will fix the batch size at 32."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3EqIU-esygna"
      },
      "outputs": [],
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "    rescale = 1./255,\n",
        "    rotation_range = 15,\n",
        "    horizontal_flip = True,\n",
        "    zoom_range = 0.2,\n",
        "    shear_range = 0.1,\n",
        "    fill_mode = 'reflect',\n",
        "    width_shift_range = 0.1,\n",
        "    height_shift_range = 0.1\n",
        ")\n",
        "\n",
        "train_generator = train_datagen.flow_from_dataframe(\n",
        "    train_df,\n",
        "    \"./train\",\n",
        "    x_col = 'filename',\n",
        "    y_col = 'category',\n",
        "    target_size = (img_hieght,img_width),\n",
        "    batch_size = batch_size,\n",
        "    class_mode = 'binary'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WJUSlNIygna"
      },
      "source": [
        "**Test Generator**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvvDoGzyygnb"
      },
      "source": [
        "We will not apply any data augmentation technique like horizontal flip, zoom etc on validation set because we are not going to use this for training purpose."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CwsDUGcdygnb"
      },
      "outputs": [],
      "source": [
        "test_datagen = ImageDataGenerator(\n",
        "    rescale = 1./255,\n",
        ")\n",
        "\n",
        "test_generator = test_datagen.flow_from_dataframe(\n",
        "    test_df,\n",
        "    \"./train\",\n",
        "    x_col = 'filename',\n",
        "    y_col = 'category',\n",
        "    target_size = (img_hieght,img_width),\n",
        "    batch_size = batch_size,\n",
        "    class_mode = 'binary'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWZzftiwygnb"
      },
      "source": [
        "## Building Your First ResNet Model (50 layers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4L3RJFdpygnb"
      },
      "source": [
        "In this section, we can develop a convolutional neural network model for the dogs vs. cats dataset.\n",
        "\n",
        "*Layers needed by Resnet*\n",
        "\n",
        "* Conv2D :- Basic Convolutional layer which is used to extract features from our image.\n",
        "* MaxPooling :- CNN has a concept of max pooling. After every convoulution we get some values in a kernel. However in max pooling we select max kernel value.It helps us to reduce unnecessary noise from our data and keep only useful values for training.\n",
        "* BatchNormalization:- This layer helps for normalization of our input values which helps in fast learning of our model.\n",
        "\n",
        "* Dense:- Dense layer is needed by every neural network to finally output the result however every once in while using a Dense layer helps in making model learn.\n",
        "*  Flatten:- Conv2D layer returns doesn't return a flatten data hence we need Flatten layer before feeding it into final Dense layer.\n",
        "\n",
        "You'll need to use this function: \n",
        "- Average pooling [see reference](https://www.tensorflow.org/api_docs/python/tf/keras/layers/AveragePooling2D)\n",
        "\n",
        "Here are some other functions we used in the code below:\n",
        "- Conv2D: [See reference](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D)\n",
        "- BatchNorm: [See reference](https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization)\n",
        "- Max pooling: [See reference](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D)\n",
        "- Fully connected layer: [See reference](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense)\n",
        "- Addition: [See reference](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Add)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Epl5kbTgygnb"
      },
      "source": [
        "<a href=\"https://ibb.co/vxv4Kr0\"><img src=\"https://i.ibb.co/d0J4z38/resnet.png\" alt=\"resnet\" border=\"0\" /></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QT3-vvfOygnb"
      },
      "source": [
        "This image is just an example of cnn model, it not describes our exact model\n",
        "\n",
        "**Important part**\n",
        "\n",
        "The details of this ResNet-50 model are:\n",
        "\n",
        "    Stage 1:\n",
        "        The 2D Convolution has 64 filters of shape (7,7) and uses a stride of (2,2).\n",
        "        BatchNorm is applied.\n",
        "        MaxPooling uses a (3,3) window and a (2,2) stride.\n",
        "    Stage 2:\n",
        "        The convolutional block uses three sets of filters of size [64,64,256,256]\n",
        "             1. 64 for conv 1x1 and (2,2) stride in First component of main path \n",
        "             2. 64 for conv 3x3 in Second component of main path\n",
        "             3. 256 for conv 1x1 in Third component of main path \n",
        "             4. 256 for conv 1x1 and (2,2) stride in SHORTCUT PATH.\n",
        "        The 2 identity blocks use three sets of filters of size [64,64,256].\n",
        "    Stage 3:\n",
        "        The convolutional block uses three sets of filters of size [128,128,512,512].\n",
        "             1. 128 for conv 1x1 and (2,2) stride in First component of main path \n",
        "             2. 128 for conv 3x3 in Second component of main path\n",
        "             3. 512 for conv 1x1 in Third component of main path \n",
        "             4. 512 for conv 1x1 and (2,2) stride in SHORTCUT PATH.\n",
        "        The 3 identity blocks use three sets of filters of size [128,128,512].\n",
        "    Stage 4:\n",
        "        The convolutional block uses three sets of filters of size [256, 256, 1024,1024].\n",
        "             1. 256 for conv 1x1 and (2,2) stride in First component of main path \n",
        "             2. 256 for conv 3x3 in Second component of main path\n",
        "             3. 1024 for conv 1x1 in Third component of main path \n",
        "             4. 1024 for conv 1x1 and (2,2) stride in SHORTCUT PATH.\n",
        "        The 5 identity blocks use three sets of filters of size [256, 256, 1024].\n",
        "    Stage 5:\n",
        "        The convolutional block uses three sets of filters of size [512, 512, 2048,2048].\n",
        "             1. 512 for conv 1x1 and (2,2) stride in First component of main path \n",
        "             2. 512 for conv 3x3 in Second component of main path\n",
        "             3. 2048 for conv 1x1 in Third component of main path \n",
        "             4. 2048 for conv 1x1 and (2,2) stride in SHORTCUT PATH.  \n",
        "        The 2 identity blocks use three sets of filters of size [512, 512, 2048].\n",
        "    The 2D Average Pooling uses a pool_size=(2, 2).\n",
        "    The 'flatten' layer doesn't have any hyperparameters.\n",
        "    The Fully Connected (Dense) layer reduces its input to the number of classes using a softmax activation.\n",
        "\n",
        "**Noted From stage 2 to stage 5 use (2,2) stride for 2D Convolution of First component of main path and SHORTCUT PATH.** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-W4hzCsygnc"
      },
      "source": [
        "## Exercise - ResNet50\n",
        "### Let’s Build ResNet from scratch:\n",
        "#####  It's showtime, guys!!!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JZHfea4ygnc"
      },
      "source": [
        "<a href=\"https://ibb.co/VHtkfj4\"><img src=\"https://i.ibb.co/M8sJF20/1-ch-Cna4p3-A09-VDC5hg-Llf-Hg.png\" alt=\"1-ch-Cna4p3-A09-VDC5hg-Llf-Hg\" border=\"0\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PtSu9rSYygnc"
      },
      "outputs": [],
      "source": [
        "input = layers.Input(shape=((img_hieght,img_width,img_channels)),name ='Input_Image')\n",
        "\n",
        "x = layers.ZeroPadding2D((3, 3))(input)\n",
        "# Stage 1\n",
        "x = layers.Conv2D(64, (7, 7), strides = (2, 2))(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Activation('relu')(x)\n",
        "x = layers.MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
        "\n",
        "# Stage 2\n",
        "\n",
        "## Convolotion Block\n",
        "# Save the input value\n",
        "x_shortcut = x\n",
        "\n",
        "# First component of main path \n",
        "x = layers.Conv2D(filters = 64, kernel_size = 1, strides = (1, 1), padding='valid')(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Activation('relu')(x)\n",
        "    \n",
        "## Second component of main path \n",
        "x = layers.Conv2D(filters = 64, kernel_size = 3,strides = (1, 1),padding='same')(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Activation('relu')(x)\n",
        "\n",
        "## Third component of main path \n",
        "x = layers.Conv2D(filters = 256, kernel_size = 1, strides = (1, 1), padding='valid')(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "    \n",
        "##### SHORTCUT PATH ##### \n",
        "x_shortcut = layers.Conv2D(filters = 256 , kernel_size = 1, strides = (1,1), padding='valid')(x_shortcut)\n",
        "x_shortcut = layers.BatchNormalization()(x_shortcut)\n",
        "    \n",
        "# Final step: Add shortcut value to main path (Use this order [x, x_shortcut]), and pass it through a RELU activation\n",
        "x = layers.Add()([x, x_shortcut])\n",
        "x = layers.Activation('relu')(x)\n",
        "\n",
        "### START CODE HERE\n",
        "\n",
        "## identity_block 1\n",
        "# Save the input value.\n",
        "\n",
        "# First component of main path \n",
        "\n",
        "## Second component of main path\n",
        "\n",
        "## Third component of main path \n",
        "\n",
        "## identity_block 2\n",
        "# Save the input value.\n",
        "\n",
        "# First component of main path \n",
        "\n",
        "## Second component of main path\n",
        "\n",
        "## Third component of main path \n",
        "\n",
        "# Final step: Add shortcut value to main path and pass it through a RELU activation\n",
        "\n",
        "# Stage 3\n",
        "\n",
        "\n",
        "# Stage 4\n",
        "\n",
        "\n",
        "# Stage 5\n",
        "\n",
        "\n",
        "## AVGPOOL (≈1 line). Use \"X = AveragePooling2D(...)(X)\"\n",
        "\n",
        "output = layers.Dense(1,activation='sigmoid')(x)\n",
        "### END CODE HERE\n",
        "model = keras.Model(inputs = input, outputs =output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUUZGWdbygnd"
      },
      "source": [
        "model.summary() gives the description of the architecture of our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fas6jfZ5ygnd"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGrvQomyygne"
      },
      "source": [
        "Finally we will compile the model .There are 3 things to mention here : Optimizer,Loss, Metrics\n",
        "* Optimizer :- To minimize cost function we use different methods For ex :- like gradient descent, stochastic gradient descent. So these are call optimizers. We are using a default one here which is adam.\n",
        "* Loss :- To make our model better we either minimize loss or maximize accuracy. Neural Networks always minimize loss. To measure it we can use different formulas like 'categorical_crossentropy' or 'binary_crossentropy'. Here I have used binary_crossentropy.\n",
        "* Metrics :- This is to denote the measure of your model. Can be accuracy or some other metric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ar_ZGaLyygne"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='Adam',loss='binary_crossentropy',metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FY_5bMhuygne"
      },
      "source": [
        "**Fit Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ekNApwBygne"
      },
      "source": [
        "We are now going to train our compiled model using the train iterator (train_generator) and use the val iterator (val_generator) as a validation dataset during training.The number of steps for the train and validation iterators must be specified. This is the number of batches that will comprise one epoch. This can be specified via the length of each iterator, and will be the total number of images in the train and validation directories divided by the batch size (32).The model will be fit for 10 epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I7th0Nmeygne"
      },
      "outputs": [],
      "source": [
        "model.fit(train_generator, epochs = 10, steps_per_epoch = train_size//32)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQ06fnJlygnf"
      },
      "source": [
        "**Saving the Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TtBg1cgygnf"
      },
      "source": [
        "Once fit, we can save the final model to an .h5 file by calling the save() function on the model and pass in the chosen filename."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYCXptbUygnf"
      },
      "outputs": [],
      "source": [
        "model.save(\"model_resnet.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hq9rvgblygnf"
      },
      "source": [
        "Below code can be used to load model weights from save model.h5 file. So that we don't have to go through full trainnig process again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56WKJeXZygnf"
      },
      "outputs": [],
      "source": [
        "# model = load_model(\"/kaggle/input/mode-file/model.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ED2zrv7cygnf"
      },
      "source": [
        "**Test Accuracy**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDqVhqyuygnf"
      },
      "source": [
        "Now we will calculate our model's accuracy on validation set with use of keras  *.evaluate_generator()* function. We have used score[1]*100 because output of evaluate() is a list of which second element represents accuracy of model on passed dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XFebQK_jygnf"
      },
      "outputs": [],
      "source": [
        "score = model.evaluate(test_generator)\n",
        "print(score[1]*100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2l_N1QY_ygng"
      },
      "source": [
        "**Training Accuracy**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zs63cvoNygng"
      },
      "source": [
        "Here we are going to calculate model's accuracy on training set itself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Vt_76Zrygng"
      },
      "outputs": [],
      "source": [
        "score_train = model.evaluate(train_generator)\n",
        "print(score_train [1]*100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpbdN0Q8ygng"
      },
      "source": [
        "### Woo hoo!  Congratulations\n",
        "on finishing this assignment! You've now implemented a state-of-the-art image classification system! \n",
        "\n",
        "ResNet50 is a powerful model for image classification when it's trained for an adequate number of iterations. Hopefully, from this point, you can use what you've learned and apply it to your own classification problem to perform state-of-the-art accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLjRXCaLygng"
      },
      "source": [
        "### Test on Your Own Image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQ3W9srqygnh"
      },
      "source": [
        "Here we will see the prediction of our model on a new image with use of model.predict function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LP0MBS0Vygnh"
      },
      "outputs": [],
      "source": [
        "path = \"2.jpg\"\n",
        "img = cv2.imread(path)\n",
        "plt.imshow(img)\n",
        "img = cv2.resize(img,(128,128))\n",
        "img = np.reshape(img,[1,128,128,3])\n",
        "img = np.divide(img,255)\n",
        "result = model.predict(img)\n",
        "if result[0] >= 0.5:\n",
        "    print(\"According to our model's prediction below image is of a Dog\")\n",
        "else:\n",
        "    print(\"According to our model's prediction below image is of a Cat\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCMWMjTHygnh"
      },
      "source": [
        "\n",
        "## Bibliography\n",
        "\n",
        "This notebook presents the ResNet algorithm from He et al. (2015). The implementation here also took significant inspiration and follows the structure given in the GitHub repository of Francois Chollet: \n",
        "\n",
        "- Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun - [Deep Residual Learning for Image Recognition (2015)](https://arxiv.org/abs/1512.03385)\n",
        "- Francois Chollet's GitHub repository: https://github.com/fchollet/deep-learning-models/blob/master/resnet50.py\n",
        "\n",
        "Author : Moeurn Dear"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}